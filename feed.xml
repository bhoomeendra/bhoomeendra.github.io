<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://bhooomeendra.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bhooomeendra.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-31T17:58:37+00:00</updated><id>https://bhooomeendra.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Discourse-Aware Unsupervised Summarization of Long Scientific Documents (Hiporank)</title><link href="https://bhooomeendra.github.io/blog/2023/HipoRank/" rel="alternate" type="text/html" title="Discourse-Aware Unsupervised Summarization of Long Scientific Documents (Hiporank)" /><published>2023-04-29T00:00:00+00:00</published><updated>2023-04-29T00:00:00+00:00</updated><id>https://bhooomeendra.github.io/blog/2023/HipoRank</id><content type="html" xml:base="https://bhooomeendra.github.io/blog/2023/HipoRank/"><![CDATA[<h2 id="papers-objective">Paper’s Objective</h2>
<p>The paper adapts graph-based ranking model for <strong>extractive summarization</strong> of long scientific documents by exploiting discoures structure i.e., sections in the case of scientific documents.</p>

<h2 id="problem-formulation">Problem formulation</h2>
<p>A document is represented as a set of nodes. Each node represents a sentence, and the similarity between two sentences is the edge weight between them. Summary generation is formulated as a node selection problem, in which nodes (i.e., sentences) that are semantically similar to other nodes are chosen to be included in the final summary. In other words, they determine node importance by defining a notion of centrality in the graph.</p>

<h2 id="central-idea">Central Idea</h2>
<ul>
  <li>
    <p>A simple unsupervised graph-based ranking model combined with proper sophisticated modeling of discourse information as an inductive bias can achieve unreasonable effectiveness in selecting important sentences from long scientific documents. (LexRank and PacSum are the choices of unsupervised graph-based ranking models.)</p>
  </li>
  <li>
    <p>Important information typically occurs at the start and end of sections.</p>
  </li>
  <li>
    <p>Most sentences across section boundaries are unlikely to interact significantly with each other. To implement this, a hierarchy is introduced in the graph. A section-level representations as graph nodes in addition to sentence nodes  By doing so, we convert
a flat graph into a hierarchical non-fully-connected graph, which has two advantages:</p>
    <ul>
      <li>Reduced computational cost</li>
      <li>Pruning of distracting weak</li>
    </ul>
  </li>
</ul>

<h2 id="methodology">Methodology</h2>
<p>Let us represent a document as a graph \(G=(V,E)\) where \(V\) is the set of vertices that represent sentences or other textual units in the document and \(E\) is the set of edges that represent the similarity between sentences. The directed edge \(e_{ij}\) from node \(v_i\) to node \(v_j\) is typically weighted by \(w_{ij} = f (sim(v_i , v_j))\), where \(sim\) is a measure of similarity between two nodes.</p>

<p>The algorithms select the most salient sentences from \(V\) based on the assumption that sentences that are similar to a greater number of other sentences capture more important content and therefore are more informative.</p>

<p>To create the hierarchy, we allow two levels of connections in our hierarchical graph: intra-sectional connections and inter-sectional connections.</p>

<p><strong>Intra-sectional connections</strong> aim to model the local importance of a sentence within its section. It implements the idea that a sentence that is similar to a greater number of other sentences in the same topic/section should be more important. This is realized in our fully-connected subgraph for an arbitrary section I, where we allow sentence-sentence edges for all sentence nodes within the same section.</p>

<p><strong>Inter-sectional connections</strong> aim to model the global importance of a sentence with respect to
other topics/sections in the document, as a sentence that is similar to a greater number of other topics is deemed more important.</p>

<div style="text-align: center;">
<img src="https://d3i71xaburhd42.cloudfront.net/20a3ab365ad179dcb0dc19c5dbcdede772a2bcb8/5-Figure1-1.png" width="700" height="250" class="center" />
</div>
<p> </p>

<p>The authores introduce section nodes on top of sentence nodes to form a hierarchical graph. For inter-section connections, we only allow section-sentence edges for modeling the global information.</p>

<p>To calculate the weight of an edge, we first measure similarity between a sentence-sentence pair \(sim(v_j^{I} , v_i^{I} )\) and a section-sentence pair \(sim(v^J , v_i^I )\). While our method is agnostic to the measure of similarity, we use cosine similarity with different vector representations in our experiments, averaging a section’s sentence representations to obtain its own.</p>

<p><strong>While the similarities of two graph nodes are symmetric, one may be more salient than the other when considering their discourse structures.</strong></p>

<p><strong>Asymmetric edge weighting over sentences</strong> is based on the hypothesis that important sentences are near the boundaries (start or end) of a text (Baxendale,1958). We reflect this hypothesis by defining sentence boundary function \(d_b\) over sentences \(v_i^I\)  in section I such that sentences closer to the section’s
boundaries are more important:</p>

\[d_b(v_i^I ) = min(x_i^I , α(n^I − x_i^I ))\]

<p>where \(n^I\) is the number of sentences in section \(I\) and \(x_i^I\) represents sentence i’s position in the section \(I\). \(α ∈ R+\) is a hyper-parameter that controls the relative importance of the start or end of a section or document.</p>

<p>we define the weight \(w_{ji}^I\) for intra-section edges (incoming edges for i) as:</p>

\[w_{ji}^I= 
\begin{cases}
    \lambda_1*sim(v_j^{I} , v_i^{I} ),&amp; \text{if } d_b(v_i^{I}) \geq d_b(v_j^{I})\\
    \lambda_2*sim(v_j^{I} , v_i^{I} ),&amp; \text{otherwise}
\end{cases}\]

<p>where \(λ1 &lt; λ2\) such that an edge \(e_{ji}\) incident to i is weighted more if i is closer to the text boundary than j. Edges with a weight below a certain threshold \(\beta\) can be pruned.</p>

<p><strong>Asymmetric edge weighting over sections</strong> similar equations are created for inter-section edges. The weight \(w_i^{JI}\) for an edge from section \(J\) to sentence \(i\) in section \(I\) is defined as:</p>

\[w_i^{JI}= 
\begin{cases}
    \lambda_1*sim(v^{J} , v_i^{I} ),&amp; \text{if } d_b(v^{I}) \geq d_b(v^{J})\\
    \lambda_2*sim(v^{J} , v_i^{I} ),&amp; \text{otherwise}
\end{cases}\]

<p>The same distance criteria is used for inter-section edges as well by using the position of section in the document instead of the position of sentence in the section.</p>

<p><strong>Importance Calculations</strong>
We compute the overall importance of sentence \(v_i^I\) as the weighted sum of its inter-section and intra-section centrality scores:</p>

\[c(v_i^I) = μ_1 · c_{inter}(v_i^I) + c_{intra}(v_i^I )\]

\[c_{intra}(v_i^{I}) = \sum_{v_j^I\in I}\frac{w_{ji}^I}{|I|}\]

\[c_{inter}(v_i^{I}) = \sum_{v^J\in D}\frac{w_i^{JI}}{|D|}\]

<p>where \(I\) is the set of sentences neighboring \(v_i^I\) and \(D\) is the set of neighboring sections in the hierarchical document graph; \(μ_1\) is a weighting factor for inter-section centrality.</p>

<p><strong>Summary Generation</strong></p>

<p>Lastly, we generate a summary by greedily extracting sentences with the highest importance scores until a predefined word-limit L is passed. Most graph-based ranking algorithms recompute importance after each sentence is extracted in order to prevent content overlap. However, we find that the asymmetric edge scoring functions naturally prevent redundancy because similar sentences have different boundary positional scores. Our method thus successfully extracts diverse sentences without recomputing importance.</p>

<h2 id="results-and-limitations">Results and Limitations</h2>

<p><strong>Cons</strong></p>

<ol>
  <li>
    <p>The method is limited to long documents with sections, .i.e, have a discourse structure if there are no section then we have to use some way to replace the sections with some other abstraction or we might have to go back to using the flat graph.</p>
  </li>
  <li>
    <p>Coherence will also be an issue as the method is extractive in nature and will not lead to a coherent flow of sentences.</p>
  </li>
  <li>
    <p>Domain-specific nuances might not captured by the method we might have to use modification to address this issue also it does not allow for any kind of supervision. Hence guiding the algorithm to capture domain-specific nuances would be difficult might require futher modeling.</p>
  </li>
</ol>

<p><strong>Pros</strong></p>

<ol>
  <li>
    <p>The method is unsupervised and can be applied to any domain without any need for annotated data.</p>
  </li>
  <li>
    <p>The method is extractive in nature and hence the summary is faithful to the original text and factual correctness of the intact.</p>
  </li>
  <li>
    <p>The method is competitive with the state-of-the-art abstractive summarization methods as on 2023 on arxiv Dataset and would be a strong baseline for any long summarization task with discoure structre.</p>
  </li>
</ol>

<h2 id="implication">Implication</h2>
<ol>
  <li>Extractive summarization faithfully preserves the original text hence ensuring the factual correctness of a summary to some extend. (Which is good for critical applications like medical, legal, etc.)</li>
  <li>As the method is unsupervised, it can be applied to any domain without any need for annotated data.</li>
</ol>]]></content><author><name></name></author><category term="Research" /><category term="Paper_Summary" /><category term="NLP" /><category term="Summarization" /><category term="Unsupervised" /><category term="Graph" /><summary type="html"><![CDATA[Paper Summary]]></summary></entry><entry><title type="html">(LORA) LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</title><link href="https://bhooomeendra.github.io/blog/2023/LoRa/" rel="alternate" type="text/html" title="(LORA) LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS" /><published>2023-04-29T00:00:00+00:00</published><updated>2023-04-29T00:00:00+00:00</updated><id>https://bhooomeendra.github.io/blog/2023/LoRa</id><content type="html" xml:base="https://bhooomeendra.github.io/blog/2023/LoRa/"><![CDATA[<h2 id="papers-objective">Paper’s Objective</h2>
<p>LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency.</p>

<h2 id="central-idea">Central Idea</h2>
<p>The weights of the pre-trained model are frozen for example in the self-attention layer of the encoder we have matrix \(W_q\) which is of size \(d \times k\), The hypothesis that the authors have is that instead of adjusting the weights of \(W\) we keep it fixed and learn a \(\Delta W\) which can be further decomposed into \(A\) and \(B\) such that \(\Delta W = A \times B\) where \(A\) is of size \(d \times r\) and \(B\) is of size \(r \times k\) where \(r\) is the rank of the matrix \(\Delta W\). The hypothesis here is that the gradient update matrixes are rank deficient and can be decomposed into low rank matrices. Empirically, the authors show that the very low values like \(r=1\) or \(2\) also works.</p>

<div style="text-align: center;">
<img src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*BCs63SXaAu3NKqUaTLTH2g.png" width="250" height="250" class="center" />
</div>
<p> </p>

<p>Matrix A is initialized with gaussian random values and matrix B is initialized with the zeros so intially \(\Delta W\) is zero and the model is the same as the pre-trained model. We then scale \(\Delta W x\) by where \(\alpha\) is a constant in \(r\). When optimizing with Adam, tuning \(\alpha\) is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set \(\alpha\) to the first \(r\)  we try and do not tune it.</p>

<p>When deployed in production, we can explicitly compute and store \(W = W_0 + BA\) and perform inference as usual. Note that both \(W_0\) and \(BA\) are in \mathbb{R}^{d \times k}. When we need to switch to another downstream task, we can recover \(W_0\) by subtracting \(BA\) and then adding a different \(B_0A_0\) , a quick operation with very little memory overhead.</p>]]></content><author><name></name></author><category term="Research" /><category term="Paper_Summary" /><category term="NLP" /><category term="Summarization" /><category term="Fine_Tuning" /><summary type="html"><![CDATA[Paper Summary]]></summary></entry><entry><title type="html">On Parenting</title><link href="https://bhooomeendra.github.io/blog/2023/parenting/" rel="alternate" type="text/html" title="On Parenting" /><published>2023-04-27T00:00:00+00:00</published><updated>2023-04-27T00:00:00+00:00</updated><id>https://bhooomeendra.github.io/blog/2023/parenting</id><content type="html" xml:base="https://bhooomeendra.github.io/blog/2023/parenting/"><![CDATA[<h2 id="-the-secret-to-raising-smart-kids-"><a href="https://www.scientificamerican.com/article/the-secret-to-raising-smart-kids/"> The Secret to Raising Smart Kids </a></h2>

<p>A brilliant student, Jonathan sailed through grade school. He completed his assignments easily and routinely earned As. Jonathan puzzled over why some of his classmates struggled, and his parents told him he had a special gift. In the seventh grade, however, Jonathan suddenly lost interest in school, refusing to do homework or study for tests. As a consequence, his grades plummeted. His parents tried to boost their son’s confidence by assuring him that he was very smart. But their attempts failed to motivate Jonathan.</p>

<h4 id="implication">Implication</h4>
<ul>
  <li>Jonathan implicitly learned that intelligence is innate and fixed and, striving to learn seems far less important.</li>
  <li>Jonathan associated being smart meant that one’s ability to do task with no or minimal effort</li>
  <li>Jonathan has attached his ego to his innate capabilities; hence, challenges, mistakes, and even exerting effort threaten ego rather than learning opportunities.</li>
  <li>Jonathan now losses confidence and motivation when the work is no longer easy.</li>
  <li>The correct association to build in a child is of effort and smartness. One should not care about the outcome rather putting in a honest effort. A person can’t change how their innate abilities as they also don’t have any control over them but rather on the work they put in.</li>
  <li>Associating smartness with ease or completing work with minimal effort lead us to the path of learned helplessness as the individual start to believe that they have no control over the problem and think that only talented people can do such a task.</li>
  <li>In particular, attributing poor performance to a lack of ability depresses motivation more than the belief that lack of effort is to blame.</li>
</ul>

<h4 id="two-views-of-intelligence">Two Views of Intelligence</h4>
<p>Two general classes of learners helpless and mastery-oriented.</p>

<h6 id="helpless-learnersfixed-mindset"><strong>Helpless learners:(Fixed Mindset)</strong></h6>
<p>The helpless ones believe that intelligence is a fixed trait: you have only a certain amount, and that’s that. Mistakes crack their self-confidence because they attribute errors to a lack of ability, which they feel powerless to change. They avoid challenges because challenges make mistakes more likely and looking smart less so. Like Jonathan, such children shun effort in the belief that having to work hard means they are dumb.</p>

<h6 id="mastery-oriented-learnersgrowth-mindset"><strong>Mastery-oriented learners:(Growth Mindset)</strong></h6>
<p>The mastery-oriented children, on the other hand, think intelligence is malleable and can be developed through education and hard work. They want to learn above all else. After all, if you believe that you can expand your intellectual skills, you want to do just that. Because slipups stem from a lack of effort, not ability, they can be remedied by more effort. Challenges are energizing rather than intimidating; they offer opportunities to learn.</p>

<h4 id="effect-on-relationships">Effect on Relationships</h4>
<p>Ones ability understand that people can change overtime and not labeling people in a certian way helps in build healty relationships because in such sitution when a problem surfaces it is disscused as their is a mutual understand that a problem can be worked out.</p>]]></content><author><name></name></author><category term="Advice" /><category term="Parenting" /><summary type="html"><![CDATA[This blog includes all the parenting advice which will be helpful in rasing better humans.]]></summary></entry></feed>