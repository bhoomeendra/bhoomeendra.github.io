---
layout: post
title: (LORA) LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 
date: 2023-04-29 
description: Paper Summary
tags: Paper_Summary NLP Summarization Fine_Tuning 
categories: Research
related_posts: false
---
## Paper's Objective
LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency.

## Central Idea
The weights of the pre-trained model are frozen for example in the self-attention layer of the encoder we have matrix $$W_q$$ which is of size $$d \times k $$, The hypothesis that the authors have is that instead of adjusting the weights of $$W$$ we keep it fixed and learn a $$\Delta W$$ which can be further decomposed into $$A$$ and $$B$$ such that $$\Delta W = A \times B$$ where $$A$$ is of size $$d \times r$$ and $$B$$ is of size $$r \times k$$ where $$r$$ is the rank of the matrix $$\Delta W$$. The hypothesis here is that the gradient update matrixes are rank deficient and can be decomposed into low rank matrices. Empirically, the authors show that the very low values like $$r=1$$ or $$2$$ also works.  

 <div style="text-align: center;">
<img src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*BCs63SXaAu3NKqUaTLTH2g.png"  width="250" height="250" class="center">
</div>
&nbsp;

Matrix A is initialized with gaussian random values and matrix B is initialized with the zeros so intially $$\Delta W$$ is zero and the model is the same as the pre-trained model. We then scale $$\Delta W x$$ by where $$\alpha$$ is a constant in $$r$$. When optimizing with Adam, tuning $$\alpha$$ is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set $$\alpha$$ to the first $$r$$  we try and do not tune it.

When deployed in production, we can explicitly compute and store $$W = W_0 + BA$$ and perform inference as usual. Note that both $$W_0$$ and $$BA$$ are in \mathbb{R}^{d \times k}. When we need to switch to another downstream task, we can recover $$W_0$$ by subtracting $$BA$$ and then adding a different $$B_0A_0$$ , a quick operation with very little memory overhead.